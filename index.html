<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60320583-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60320583-2');
</script>

<title>Lin Huang</title>
<link rel="stylesheet" type="text/css" href="style.css">

<script type="text/javascript" src="js/hidebib.js"></script>

</head>
<body>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="section">
  <h1>Lin Huang</h1>
</div>
<hr>

<div class="section">
  <table>
    <tr valign="top"> <td style="width: 600px; vertical-align: top;">
      I am currently a Ph.D. student at <a href = "http://www.buffalo.edu/">University at Buffalo</a>, advised by  <a href = "https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a>. I received my M.S. from the <a href = "https://www.usc.edu/">University of Southern California</a>. Prior to that, I was an undergrad at <a href = "https://en.lzu.edu.cn/">Lanzhou University</a>. 
      <p><br></p>
      I am broadly interested in computer vision and machine learning. My current research focuses on pose estimation, 3D reconstruction, human behavior analysis, and human-computer interaction.
      <p><br></p>
      <p style="text-align:center">
        <a href="mailto:lhuang27@buffalo.edu">Email</a> &nbsp|&nbsp
        <a href="./file/lin_cv.pdf">CV</a> &nbsp|&nbsp
        <a href="https://scholar.google.com/citations?user=Kwhor_EAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
        <a href="https://github.com/LinHuang17">GitHub</a> &nbsp|&nbsp
        <a href="https://linkedin.com/in/lin-huang-b898b5156">Linkedin</a>
      </p>
      <pre xml:space="preserve" id="email" style="font-size: 12px">
        
      </pre>
      <script xml:space="preserve" language="JavaScript">
  hideblock('email');
  </script>
  </td>
  
  <td width="400"><img src="./fig/Lin.jpg" alt="My picture" height=250 align="right"/></td>
</tr>
</table>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="section">
<h2> Research Projects </h2><br>
<!-- <div class="year_heading"><br>2021<hr width=800 align="left"></div> -->

<!--------------------------------------------------------------------------->
<div class="paper" id="huang2022ncf">
<!-- <img class="paper" src="./fig/ncf/ncf.jpg" height=115 /> -->
<img class="paper" src="./fig/ncf/ncf.jpg" />
<p><c1 id="papertitle">Neural Correspondence Field for Object Pose Estimation</c1> <br/> 
Lin Huang, Tomas Hodan, Lingni Ma, Linguang Zhang, Luan Tran, Christopher Twigg, Po-Chen Wu, Junsong Yuan, Cem Keskin, Robert Wang <br/> 
European Conference on Computer Vision (ECCV) 2022, Tel-Aviv <br/> 
<p><br></p>
<a href="https://arxiv.org/pdf/2208.00113.pdf">Paper</a> &nbsp|&nbsp
<a href="https://github.com/LinHuang17/NCF-code">Code</a> &nbsp|&nbsp
<a href="https://linhuang17.github.io/NCF/">Project Page</a> &nbsp|&nbsp
<a href="javascript:toggleblock('huang2022ncfAbs')">Abstract</a> &nbsp|&nbsp 
<!-- <a href="">Presentation</a> &nbsp/&nbsp -->
<!-- <a href="">Video</a> &nbsp|&nbsp  -->
<a href="javascript:toggleblock('huang2022ncfBib')">Bibtex</a>
<div class="papermeta" id="huang2022ncfMeta">
<em id="huang2022ncfAbs">We propose a method for estimating the 6DoF pose of a rigid object with an available 3D model from a single RGB image. 
  Unlike classical correspondence-based methods which predict 3D object coordinates at pixels of the input image, the proposed method predicts 
  3D object coordinates at 3D query points sampled in the camera frustum. The move from pixels to 3D points, which is inspired by recent PIFu-style 
  methods for 3D reconstruction, enables reasoning about the whole object, including its (self-)occluded parts. For a 3D query point associated with 
  a pixel-aligned image feature, we train a fully-connected neural network to predict: (i) the corresponding 3D object coordinates, and 
  (ii) the signed distance to the object surface, with the first defined only for query points in the surface vicinity. We call the mapping realized 
  by this network as Neural Correspondence Field. The object pose is then robustly estimated from the predicted 3D-3D correspondences by the Kabsch-RANSAC algorithm. 
  The proposed method achieves state-of-the-art results on three BOP datasets and is shown superior especially in challenging cases with occlusion.</em>
<pre xml:space="preserve" id="huang2022ncfBib">
@article{huang2022ncf,
  title={Neural Correspondence Field for Object Pose Estimation},
  author={Huang, Lin and Hodan, Tomas and Ma, Lingni and Zhang, Linguang and Tran, Luan and Twigg, Christopher and Wu, Po-Chen and Yuan, Junsong and Keskin, Cem and Wang, Robert},
  journal={European Conference on Computer Vision (ECCV)},
  year={2022}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('huang2022ncfAbs');
hideblock('huang2022ncfBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="HUANG2021207">
<img class="paper" src="./fig/survey/hand/hand-survey.png" height=100 />
<p><c1 id="papertitle">Survey on Depth and RGB Image-based 3D Hand Shape and Pose Estimation</c1> <br/> 
Lin Huang&#42, Boshen Zhang&#42, Zhilin Guo&#42, Yang Xiao, Zhiguo Cao, Junsong Yuan <br/> 
Virtual Reality and Intelligent Hardware (VRIH) 2021 <br/> 
<p><br></p>
<a href="https://www.sciencedirect.com/science/article/pii/S2096579621000280">Paper</a> &nbsp|&nbsp
<!-- <a href="">Project Page</a> &nbsp/&nbsp -->
<!-- <a href="">Code</a> &nbsp/&nbsp -->
<a href="javascript:toggleblock('HUANG2021207Abs')">Abstract</a> &nbsp|&nbsp 
<!-- <a href="">Presentation</a> &nbsp/&nbsp -->
<!-- <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58595-2_2">Video</a> &nbsp|&nbsp  -->
<a href="javascript:toggleblock('HUANG2021207Bib')">Bibtex</a>
<div class="papermeta" id="HUANG2021207Meta">
<em id="HUANG2021207Abs">The field of vision-based human hand three-dimensional (3D) shape and pose estimation has 
  attracted significant attention recently owing to its key role in various applications, such as natural human-computer 
  interactions. With the availability of large-scale annotated hand datasets and the rapid developments of deep neural networks (DNNs), 
  numerous DNN-based data-driven methods have been proposed for accurate and rapid hand shape and pose estimation. Nonetheless, 
  the existence of complicated hand articulation, depth and scale ambiguities, occlusions, and finger similarity remain challenging. 
  In this study, we present a comprehensive survey of state-of-the-art 3D hand shape and pose estimation approaches using RGB-D cameras. 
  Related RGB-D cameras, hand datasets, and a performance analysis are also discussed to provide a holistic view of recent achievements. 
  We also discuss the research potential of this rapidly growing field.</em>
<pre xml:space="preserve" id="HUANG2021207Bib">
@article{HUANG2021207,
  title = {Survey on depth and RGB image-based 3D hand shape and pose estimation},
  journal = {Virtual Reality & Intelligent Hardware},
  volume = {3},
  number = {3},
  pages = {207-234},
  year = {2021},
  issn = {2096-5796},
  doi = {https://doi.org/10.1016/j.vrih.2021.05.002},
  url = {https://www.sciencedirect.com/science/article/pii/S2096579621000280},
  author = {Lin Huang and Boshen Zhang and Zhilin Guo and Yang Xiao and Zhiguo Cao and Junsong Yuan},
  keywords = {Hand survey, 3D hand pose estimation, Hand shape reconstruction, Hand-object interactions, RGB-D cameras},
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('HUANG2021207Abs');
hideblock('HUANG2021207Bib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<!-- <div class="year_heading"><br>2020<hr width="800" align="left"></div> -->
<!--------------------------------------------------------------------------->
<div class="paper" id="mm20hotnet">
<img class="paper" src="./fig/hot/combine.gif" height=125 />
<!-- <p> <strong style="color:red">[New]</strong> <b id="papertitle">HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation</b> <br/>  -->
<p> <c1 id="papertitle">HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation</c1> <br/> 
Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, Junsong Yuan <br/> 
ACM International Conference on Multimedia (ACM MM) 2020, Seattle <br/> 
<p><br></p>
<a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413775">Paper</a> &nbsp|&nbsp
<!-- <a href="">Project Page</a> &nbsp/&nbsp -->
<!-- <a href="">Code</a> &nbsp/&nbsp -->
<a href="javascript:toggleblock('mm20hotnetAbs')">Abstract</a> &nbsp|&nbsp 
<a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413775">Presentation</a> &nbsp|&nbsp
<!-- <a href="">Video</a> &nbsp/&nbsp  -->
<a href="javascript:toggleblock('mm20hotnetBib')">Bibtex</a>
<div class="papermeta" id="mm20hotnetMeta">
<em id="mm20hotnetAbs">
  As we use our hands frequently in daily activities, the analysis of hand-object interactions plays a critical role to many 
  multimedia understanding and interaction applications. Different from conventional 3D hand-only and object-only pose estimation, 
  estimating 3D hand-object pose is more challenging due to the mutual occlusions between hand and object, as well as the physical 
  constraints between them. To overcome these issues, we propose to fully utilize the structural correlations among hand joints and 
  object corners in order to obtain more reliable poses. Our work is inspired by structured output learning models in sequence 
  transduction field like Transformer encoder-decoder framework. Besides modeling inherent dependencies from extracted 2D hand-object 
  pose, our proposed Hand-Object Transformer Network (HOT-Net) also captures the structural correlations among 3D hand joints and 
  object corners. Similar to Transformer’s autoregressive decoder, by considering structured output patterns, this helps better 
  constrain the output space and leads to more robust pose estimation. However, different from Transformer’s sequential modeling 
  mechanism, HOT-Net adopts a novel non-autoregressive decoding strategy for 3D hand-object pose estimation. Specifically, our model
  removes the Transformer’s dependence on previously generated results and explicitly feeds a reference 3D hand-object pose into the 
  decoding process to provide equivalent target pose patterns for parallely localizing each 3D keypoint. To further improve physical 
  validity of estimated hand pose, besides anatomical constraints, we propose a cooperative pose constraint, aiming to enable the 
  hand pose to cooperate with hand shape, to generate hand mesh. We demonstrate real-time speed and state-of-the-art performance on
  benchmark hand-object datasets for both 3D hand and object poses.</em>
<pre xml:space="preserve" id="mm20hotnetBib">
@inproceedings{huang2020hot,
  title={HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation},
  author={Huang, Lin and Tan, Jianchao and Meng, Jingjing and Liu, Ji and Yuan, Junsong},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={3136--3145},
  year={2020}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('mm20hotnetAbs');
hideblock('mm20hotnetBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="huang2020hand">
<img class="paper" src="./fig/hand-tfer/hand-tfer.png" height=100 />
<p><c1 id="papertitle">Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation</c1> <br/> 
Lin Huang, Jianchao Tan, Ji Liu, Junsong Yuan <br/> 
European Conference on Computer Vision (ECCV) 2020, Glasgow <br/> 
<p><br></p>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700018.pdf">Paper</a> &nbsp|&nbsp
<!-- <a href="">Project Page</a> &nbsp/&nbsp -->
<!-- <a href="">Code</a> &nbsp/&nbsp -->
<a href="javascript:toggleblock('huang2020handAbs')">Abstract</a> &nbsp|&nbsp 
<!-- <a href="">Presentation</a> &nbsp/&nbsp -->
<a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58595-2_2">Video</a> &nbsp|&nbsp 
<a href="javascript:toggleblock('huang2020handBib')">Bibtex</a>
<div class="papermeta" id="huang2020handMeta">
<em id="huang2020handAbs">3D hand pose estimation is still far from a well-solved problem mainly due to the highly nonlinear 
  dynamics of hand pose and the difficulties of modeling its inherent structural dependencies. To address this issue, we 
  connect this structured output learning problem with the structured modeling framework in sequence transduction field. Standard 
  transduction models like Transformer adopt an autoregressive connection to capture dependencies from previously generated tokens
   and further correlate this information with the input sequence in order to prioritize the set of relevant input tokens for 
   current token generation. To borrow wisdom from this structured learning framework while avoiding the sequential modeling for 
   hand pose, taking a 3D point set as input, we propose to leverage the Transformer architecture with a novel non-autoregressive 
   structured decoding mechanism. Specifically, instead of using previously generated results, our decoder utilizes a reference 
   hand pose to provide equivalent dependencies among hand joints for each output joint generation. By imposing the reference 
   structural dependencies, we can correlate the information with the input 3D points through a multi-head attention mechanism, 
   aiming to discover informative points from different perspectives, towards each hand joint localization. We demonstrate our 
   model’s effectiveness over multiple challenging hand pose datasets, comparing with several state-of-the-art methods.</em>
<pre xml:space="preserve" id="huang2020handBib">
@inproceedings{huang2020hand,
  title={Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation},
  author={Huang, Lin and Tan, Jianchao and Liu, Ji and Yuan, Junsong},
  booktitle={European Conference on Computer Vision},
  pages={17--33},
  year={2020},
  organization={Springer}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('huang2020handAbs');
hideblock('huang2020handBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="paper" id="cai2020learning">
<img class="paper" src="./fig/motion_pred/pred_tfer_dict.png" />
<p><c1 id="papertitle">Learning Progressive Joint Propagation for Human Motion Prediction</c1> <br/> 
Yujun Cai, Lin Huang, Yiwei Wang, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Xu Yang, Yiheng Zhu, Xiaohui Shen, Ding Liu, Jing Liu, Nadia Magnenat Thalmann <br/> 
European Conference on Computer Vision (ECCV) 2020, Glasgow <br/> 
<p><br></p>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520222.pdf">Paper</a> &nbsp|&nbsp
<!-- <a href="">Project Page</a> &nbsp/&nbsp -->
<!-- <a href="">Code</a> &nbsp/&nbsp -->
<a href="javascript:toggleblock('cai2020learningAbs')">Abstract</a> &nbsp|&nbsp 
<!-- <a href="">Presentation</a> &nbsp/&nbsp -->
<!-- <a href="">Video</a> &nbsp/&nbsp  -->
<a href="javascript:toggleblock('cai2020learningBib')">Bibtex</a>
<div class="papermeta" id="cai2020learningMeta">
<em id="cai2020learningAbs">  Despite the great progress in human motion prediction, it remains a challenging task due to the 
  complicated structural dynamics of human behaviors. In this paper, we address this problem in three aspects. First, to 
  capture the long-range spatial correlations and temporal dependencies, we apply a transformer-based architecture with the 
  global attention mechanism. Specifically, we feed the network with the sequential joints encoded with the temporal 
  information for spatial and temporal explorations. Second, to further exploit the inherent kinematic chains for better 3D 
  structures, we apply a progressive-decoding strategy, which performs in a central-to-peripheral extension according to the 
  structural connectivity. Last, in order to incorporate a general motion space for high-quality prediction, we build a 
  memory-based dictionary, which aims to preserve the global motion patterns in training data to guide the predictions. We 
  evaluate the proposed method on two challenging benchmark datasets (Human3.6M and CMU-Mocap). Experimental results show 
  our superior performance compared with the state-of-the-art approaches.</em>
<pre xml:space="preserve" id="cai2020learningBib">
@inproceedings{cai2020learning,
  title={Learning progressive joint propagation for human motion prediction},
  author={Cai, Yujun and Huang, Lin and Wang, Yiwei and others},
  booktitle={European Conference on Computer Vision},
  pages={226--242},
  year={2020},
  organization={Springer}
}</pre></td>
<script language="javascript" type="text/javascript" xml:space="preserve">
hideblock('cai2020learningAbs');
hideblock('cai2020learningBib');
</script>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<!-- <div class="year_heading"><br>2019<hr width="800" align="left"></div> -->
<!--------------------------------------------------------------------------->
<div class="paper" id="mm20hotnet">
  <img class="paper" src="./fig/hpn-demo/demo.gif" height=90 />
  <p><c1 id="papertitle">Simple Demo for Hand PointNet-based Real-Time 3D Hand Pose Estimation</c1> <br/> 
  Lin Huang, Pranav Sankhe, Yiheng Li<br/> 
  <p><br></p>
  <!-- <a href="">PDF</a> &nbsp/&nbsp -->
  <!-- <a href="">Project Page</a> &nbsp/&nbsp -->
  <a href="https://github.com/LinHuang17/hand-pointnet-demo">Code</a>
  <!-- <a href="">Abstract</a> -->
  <!-- <a href="">Presentation</a> &nbsp/&nbsp -->
  <!-- <a href="">Video</a> &nbsp/&nbsp  -->
  <!-- <a href="">Bibtex</a> -->
  <div class="papermeta" id="mm20hotnetMeta">
  </div>
  </div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="section">
  <h2> Experiences</h2>
  
  <div class="paper" id="teaching">
  <table width="100%" valign="top" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td  style="width: 700px; vertical-align: top">
        </a><b id="papertitle3"></b>Research Intern: Microsoft Azure AI, Redmond, WA, USA</b></a><br />
        </a><b id="papertitle3"></b>Supervisor: Dr. Chung-Ching Lin, Dr. Kevin Lin, Dr. Lin Liang, Dr. Lijuan Wang, Dr. Zicheng Liu</b></a><br />
        <heading>May. 2022 - Aug. 2022</heading>
      <p><br/></p>
        </a><b id="papertitle3"></b>Part-Time Student Researcher: Reality Labs at Meta, Redmond, WA, USA</b></a><br />
        </a><b id="papertitle3"></b>Supervisor: Dr. Tomas Hodan</b></a><br />
        <heading>Dec. 2021 - Apr. 2022</heading>
      <p><br/></p>
        </a><b id="papertitle3"></b>Research Intern: Reality Labs at Meta, Redmond, WA, USA</b></a><br />
        </a><b id="papertitle3"></b>Supervisor: Dr. Tomas Hodan</b></a><br />
        <heading>Aug. 2021 - Dec. 2021</heading>
      <p><br/></p>
        </a><b id="papertitle3"></b>Research Intern: Y-tech Lab at Kwai, Seattle, WA, USA</b></a><br />
        </a><b id="papertitle3"></b>Supervisor: Dr. Jianchao Tan, Dr. Ji Liu</b></a><br />
        <heading>May. 2020 - Aug. 2020</heading>
      <p><br/></p>
      </td>
    </tr>
  </table>
</div>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="section">
<h2> Services</h2>

<div class="paper" id="teaching">
<table width="100%" valign="top" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td  style="width: 700px; vertical-align: top">
      <p> <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
      </a><b id="papertitle3"></b>Reviewer: CVPR, ECCV, ICCV, TIP</b></a><br />
    </td>
  </tr>
</table>
</div>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<div class="section">
  <h2> Teaching</h2>
  
  <div class="paper" id="teaching">
    <table width="100%" valign="top" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td  style="width: 700px; vertical-align: top">
      </a><b id="papertitle3"></b>TA: Introduction to Pattern Recognition (CSE555)</b></a><br />
      <heading>Fall 2020</heading>
    <p><br/></p>
      </a><b id="papertitle2"></b>TA: Data Intensive Computing (CSE587)</b></a><br />
      <heading>Spring 2020</heading>
    <p><br/></p>
    </td>
  </tr>
</table>
</div>
</div>
</div>
<!--------------------------------------------------------------------------->

<!--------------------------------------------------------------------------->
<p style="text-align:right;font-size:small;">
</a><a href="https://shubhtuls.github.io/">Website Template</a> </a><br />

</body>
</html>