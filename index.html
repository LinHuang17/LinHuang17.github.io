<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GXFVK5T1RC"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GXFVK5T1RC');
</script>

  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    a {
    /* color: #b60a1c; */
    color: #1772d0;
    /* color: #bd0a36; */
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /* font-family: 'Roboto', sans-serif; */
    /*font-family: 'Avenir Next';*/
    font-family: Georgia, serif;
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 500;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 16px;
    font-weight:450;
    }
    name {
    /* font-family: 'Lato', Verdana, Helvetica, sans-serif; */
    /* font-family: 'Roboto', sans-serif; */
    /* font-family: 'Avenir Next'; */
    font-family: Georgia, serif;
    font-size: 35px;
    font-weight: 500;
    }
    .links
    {
    font-size: 13px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="media/preview.jpg"> -->
  <title>LinHuang - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="925" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="600" valign="middle">
        <p align="center">
          <name>Lin Huang</name>
        </p>
        <p>
            I am a final-year Ph.D. candidate at <a href = "http://www.buffalo.edu/">University at Buffalo</a>, advised by  <a href = "https://cse.buffalo.edu/~jsyuan/">Prof. Junsong Yuan</a>. I received my M.S. from the <a href = "https://www.usc.edu/">University of Southern California</a>. Prior to that, I was an undergrad at <a href = "https://en.lzu.edu.cn/">Lanzhou University</a>. 
        </p>  
        <p>
            I am broadly interested in computer vision and machine learning. My current research focuses on pose estimation, 3D reconstruction, human behavior analysis, and human-computer interaction.
        </p>
        <br>
        <p align=center>
          <a href="mailto:lhuang27@buffalo.edu">Email</a> &nbsp|&nbsp
          <a href="./file/lin_cv.pdf">CV</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=Kwhor_EAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/LinHuang17">GitHub</a> &nbsp|&nbsp
          <a href="https://linkedin.com/in/lin-huang-b898b5156"> LinkedIn</a>
        </p>
        </td>
        <td width="400">
          <img src="./fig/Lin.jpg" width="250" alt="headshot">
        </td>
      </tr>
      </table>

    




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!-- <tr>
        <td>
          <heading>News</heading>
            <ul>
              <li><strong>10/2022</strong> Invited to give a talk on large-scale scene reconstruction with NeRF at <a href="https://www.computationalimaging.org/">Stanford University</a> (<a href="files/talk_stanford.pdf">Slides</a>). </li>
              <li><strong>09/2022</strong> Our paper <a href="https://niujinshuchong.github.io/monosdf/"><strong>MonoSDF</strong></a> is accepted to <strong>NeurIPS 2022</strong>. </li>
              <li><strong>09/2022</strong> Invited to give a talk on neural rendering at <a href="https://research.adobe.com/">Adobe Research</a> (<a href="files/talk_adobe.pdf">Slides</a>). </li>
              <li><strong>06/2022</strong> This summer I will be a research intern at <a href="https://research.google">Google Research</a>. </li>
              <li><strong>06/2022</strong> <strong><span style="color:#ff0000;">1st place winner</span></strong> in partial object recovery and 2nd place overall of <a href="https://cvi2.uni.lu/sharp2022/">SHARP Challenge</a>! Congratulations on my students Lei, Zhizheng, Weining, and Liudi from 3D Vision course project at ETH Zurich!
              <li><strong>05/2022</strong> Selected as an <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at CVPR 2022. </li>
              <li><strong>03/2022</strong> Our paper <img src="media/nice-slam/like.png" width="20"><a href="https://pengsongyou.github.io/nice-slam"><strong>NICE-SLAM</strong></a> is accepted to <strong>CVPR 2022</strong>! </li>
              <li><strong>02/2022</strong> Invited to talk about <a href="https://pengsongyou.github.io/sap">Shape As Points</a> at <a href="https://twitter.com/talking_papers">Talking Papers Podcast</a>. Great chat with <a href="https://www.itzikbs.com/">Yizhak Ben-Shabat</a>!</li>
              <li><strong>12/2021</strong> Gave a talk again this year at <a href="http://games-cn.org/games-webinar-20211230-214/">GAMES Seminar Series</a> on <a href="https://pengsongyou.github.io/sap">Shape As Points</a>.</li>
              <li><strong>09/2021</strong> Our <a href="https://pengsongyou.github.io/sap">Shape As Points</a> is accepted to NeurIPS 2021 as <span style="color:#ff0000;"><strong>oral presentation</strong></span> <strong>(top 0.6%)</strong>!</li>
              <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;">
              
              <li><strong>08/2021</strong> Join <a href="https://research.fb.com/category/augmented-reality-virtual-reality/">Facebook Reality Labs (FRL)</a> as a research intern this fall.</li>
              <li><strong>07/2021</strong> Two papers (<a href="https://moechsle.github.io/unisurf/">UNISURF</a> and <a href="https://creiser.github.io/kilonerf/">KiloNeRF</a>) are accepted to ICCV 2021! </li>    
              <li><strong>06/2021</strong> Gave a talk at <a href="http://games-cn.org/games-webinar-20210621-187/">GAMES Seminar Series</a> on <a href="files/Towards_Practical_Application_of_NeRF.pdf">Towards Practical Applications of NeRF</a>. </li>
              <li><strong>11/2020</strong>: A <a href="https://github.com/dsvilarkovic/dynamic_plane_convolutional_onet">master course project</a> that I advised on got accepted to WACV 2021.</li>
              <li><strong>08/2020</strong>: Start my 1-year stay at <a href="http://www.cvlibs.net/">Autonomous Vision Group (AVG)</a> at MPI Tübingen.</li>
              <li><strong>07/2020</strong> Our paper <a href="https://arxiv.org/abs/2003.04618">Convolutional Occupancy Networks</a> is accepted to ECCV 2020 as <strong>spotlight (top 5%)</strong>! </li>
              <li><strong>09/2019</strong> Start PhD journey at <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>!</li>

              <li><strong>07/2019</strong> Our paper <a href="https://arxiv.org/abs/1811.03264">Calibration Wizard</a> is accepted to ICCV 2019 as <strong>oral presentation (top 4.6%) </strong>.</li>
              <li><strong>06/2019</strong>: The extension of my master thesis got accepted to TPAMI 2019! 
            </div></div>
            </ul>
        </td>
      </tr> -->
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Projects</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr>  
          <td width="25%">
            <div class="one">
            <img src='./fig/ncf/ncf.jpg' width="200" height="117">
            </div>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Neural Correspondence Field for Object Pose Estimation
                </papertitle>
          <br>
              Lin Huang,
              Tomas Hodan, 
              Lingni Ma, 
              Linguang Zhang, 
              Luan Tran, 
              Christopher Twigg, 
              Po-Chen Wu, 
              Junsong Yuan, Cem Keskin, 
              Robert Wang
          <br>
            European Conference on Computer Vision (ECCV) 2022, Tel-Aviv
            <br>
            <br>
            <div class="links">
                [<a href="https://arxiv.org/pdf/2208.00113.pdf">PAPER</a>]&nbsp
                [<a href="https://github.com/LinHuang17/NCF-code">CODE</a>]&nbsp
                [<a href="https://linhuang17.github.io/NCF/">WEBPAGE</a>]&nbsp
                [<a href="./bib/huang2022ncf.txt">BIB</a>]&nbsp
            </div>
          </td>
        </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr>  
          <td width="25%">
            <div class="one">
            <img src='./fig/survey/hand/hand-survey.png' width="200" height="117">
            </div>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Survey on Depth and RGB Image-based 3D Hand Shape and Pose Estimation
                </papertitle>
          <br>
              Lin Huang&#42, 
              Boshen Zhang&#42, 
              Zhilin Guo&#42, 
              Yang Xiao, 
              Zhiguo Cao, 
              Junsong Yuan
          <br>
              Virtual Reality and Intelligent Hardware (VRIH) 2021
            <br>
            <br><br>
            <div class="links">
                [<a href="https://www.sciencedirect.com/science/article/pii/S2096579621000280">PAPER</a>]&nbsp
                [<a href="./bib/HUANG2021207.txt">BIB</a>]&nbsp
            </div>
          </td>
        </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr>  
          <td width="25%">
            <div class="one">
            <img src='./fig/hot/combine.gif' width="200" height="117">
            </div>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation
                </papertitle>
          <br>
              Lin Huang, 
              Jianchao Tan, 
              Jingjing Meng, 
              Ji Liu, 
              Junsong Yuan
          <br>
              ACM International Conference on Multimedia (ACM MM) 2020, Seattle
            <br>
            <br><br>
            <div class="links">
                [<a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413775">PAPER</a>]&nbsp
                [<a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413775">SLIDES</a>]&nbsp
                [<a href="./bib/huang2020hot.txt">BIB</a>]&nbsp
            </div>
          </td>
        </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr>  
          <td width="25%">
            <div class="one">
            <img src='./fig/hand-tfer/hand-tfer.png' width="200" height="117">
            </div>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation
                </papertitle>
          <br>
              Lin Huang, 
              Jianchao Tan, 
              Ji Liu, 
              Junsong Yuan
          <br>
              European Conference on Computer Vision (ECCV) 2020, Glasgow
            <br>
            <br><br>
            <div class="links">
                [<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700018.pdf">PAPER</a>]&nbsp
                [<a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58595-2_2">VIDEO</a>]&nbsp
                [<a href="./bib/huang2020hand.txt">BIB</a>]&nbsp
            </div>
          </td>
        </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr>  
          <td width="25%">
            <div class="one">
            <img src='./fig/motion_pred/pred_tfer_dict.png' width="200" height="117">
            </div>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Learning Progressive Joint Propagation for Human Motion Prediction
                </papertitle>
          <br>
          Yujun Cai, 
          Lin Huang, 
          Yiwei Wang, 
          Tat-Jen Cham, 
          Jianfei Cai, 
          Junsong Yuan, 
          Jun Liu, 
          Xu Yang, 
          Yiheng Zhu, 
          Xiaohui Shen, 
          Ding Liu, 
          Jing Liu, 
          Nadia Magnenat Thalmann
          <br>
              European Conference on Computer Vision (ECCV) 2020, Glasgow
            <br>
            <br>
            <div class="links">
                [<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520222.pdf">PAPER</a>]&nbsp
                [<a href="./bib/cai2020learning.txt">BIB</a>]&nbsp
            </div>
          </td>
        </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
    
        <tr>  
          <td width="25%">
            <div class="one">
            <img src='./fig/hpn-demo/demo.gif' width="200" height="117">
            </div>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Simple Demo for Hand PointNet-based Real-Time 3D Hand Pose Estimation
                </papertitle>
          <br>
              Lin Huang, 
              Pranav Sankhe, 
              Yiheng Li
            <br>
            <br><br><br>
            <div class="links">
                [<a href="https://github.com/LinHuang17/hand-pointnet-demo">CODE</a>]&nbsp 
            </div>
          </td>
        </tr>


      
      <!-- Industry Experiences -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Experiences</heading>
          <tr>
            <td>
            </a><b></b>Research Intern: Microsoft Azure AI, Redmond, WA, USA</b></a><br />
            </a><b></b>Supervisor: Dr. Chung-Ching Lin, Dr. Kevin Lin, Dr. Lin Liang, Dr. Lijuan Wang, Dr. Zicheng Liu</b></a><br />
            May. 2022 - Aug. 2022
            <p></p>
            </a><b></b>Part-Time Student Researcher: Reality Labs at Meta, Redmond, WA, USA</b></a><br />
            </a><b></b>Supervisor: Dr. Tomas Hodan</b></a><br />
            Dec. 2021 - Apr. 2022
            <p></p>
            </a><b></b>Research Intern: Reality Labs at Meta, Redmond, WA, USA</b></a><br />
            </a><b></b>Supervisor: Dr. Tomas Hodan</b></a><br />
            Aug. 2021 - Dec. 2021
            <p></p>
            </a><b></b>Research Intern: Y-tech Lab at Kwai, Seattle, WA, USA</b></a><br />
            </a><b></b>Supervisor: Dr. Jianchao Tan, Dr. Ji Liu</b></a><br />
            May. 2020 - Aug. 2020
            <p></p>
            </td>
          </tr>
      </table>

      <!-- Academic Services -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Services</heading>
          <tr>
            <td>
                Conference Reviewer: CVPR, ECCV, ICCV, ICIP, WF-IOT <br>
                <p></p>
                Journal Reviewer: TIP, TCSVT, TVCJ, JVCI, MVAP, SPIC
            </td>
          </tr>
      </table>

      <!-- Teaching -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Teaching</heading>
          <tr>
            <td>
                </a><b></b>TA: Introduction to Pattern Recognition (CSE555)</b></a><br />
                Fall 2020
                <p></p>
                </a><b></b>TA: Data Intensive Computing (CSE587)</b></a><br />
                Spring 2020
            </td>
          </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          <a href="https://jonbarron.info/"><font size="2">Website Template</font></a>
          <br>
          Last updated: Dec. 2022
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
